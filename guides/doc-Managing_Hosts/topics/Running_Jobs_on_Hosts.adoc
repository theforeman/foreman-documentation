[[Running_Jobs_on_Hosts]]
== Running Jobs on Hosts

You can run jobs on hosts remotely from {SmartProxies} using shell scripts or Ansible tasks and playbooks. This is referred to as remote execution.

For custom Ansible roles that you create, or roles that you download, you must install the package containing the roles on the {SmartProxy} base operating system. Before you can use Ansible roles, you must import the roles into {Project} from the {SmartProxy} where they are installed.

Communication occurs through {SmartProxyServer}, which means that {ProjectServer} does not require direct access to the target host, and can scale to manage many hosts. Remote execution uses the SSH service that must be enabled and running on the target host. Ensure that the remote execution {SmartProxy} has access to port 22 on the target hosts.

{Project} uses ERB syntax job templates. For more information, see xref:appe-{Project_Link}-Managing_Hosts-Template_Writing_Reference[].

Several job templates for shell scripts and Ansible are included by default. For more information, see xref:sect-Managing_Hosts-Setting_up_Job_Templates[].

By default, {ProjectServer} is configured to use the Katello Agent rather than remote execution. To change this setting, navigate to *Administer* > *Settings*, click *Content*, and change the *Use remote execution by default* setting.

[NOTE]
====
Any {SmartProxyServer} base operating system is a client of {ProjectServer}'s internal {SmartProxy}, and therefore this section applies to any type of host connected to {ProjectServer}, including {SmartProxyServer}s.
====

You can run jobs on multiple hosts at once, and you can use variables in your commands for more granular control over the jobs you run. You can use host facts, smart class parameters, smart variables, and host parameters to populate variable values. For more information about Ansible variables, see {BaseURL}administering_red_hat_satellite/chap-red_hat_satellite-administering_red_hat_satellite-managing_ansible_roles#sect-{Project_Link}-Administering_{Project_Link}-Managing_Ansible_Variables_with_{Project}[Overriding Ansible Variables in {Project}].
In addition, you can specify custom values for templates when you run the command. For more information, see xref:sect-Managing_Hosts-Executing_Remote_Jobs[].

[[sect-Managing_Hosts-Choosing_a_Satellite_Capsule_for_Remote_Execution]]
=== How {Project} Selects {SmartProxy} for Remote Execution
When you run a remote job on hosts, for every host, {Project} performs the following actions to find a remote execution {SmartProxy} to use. {Project} searches only for {SmartProxies} that have the remote execution feature enabled.

. {Project} finds the host's interfaces that have the *Remote execution* check box selected.
. {Project} finds the subnets of these interfaces.
. {Project} finds remote execution {SmartProxies} assigned to these subnets.
. From this set of {SmartProxies}, {Project} selects the {SmartProxy} that has the least number of running jobs. By doing this, {Project} ensures that the jobs load is balanced between remote execution {SmartProxies}.
. If {Project} does not find a remote execution {SmartProxy} at this stage, and if the *Fallback to Any {SmartProxy}* setting is enabled, {Project} adds another set of {SmartProxies} to select the remote execution {SmartProxy} from. {Project} selects the most lightly loaded {SmartProxy} from the following types of {SmartProxies} that are assigned to the host:
+
* DHCP, DNS and TFTP {SmartProxies} assigned to the host's subnets
* DNS {SmartProxy} assigned to the host's domain
* Realm {SmartProxy} assigned to the host's realm
* Puppet Master {SmartProxy}
* Puppet CA {SmartProxy}
* OpenSCAP {SmartProxy}

+
. If {Project} does not find a remote execution {SmartProxy} at this stage, and if the *Enable Global {SmartProxy}* setting is enabled, {Project} selects the most lightly loaded remote execution {SmartProxy} from the set of all {SmartProxies} in the host's organization and location to execute a remote job.

==== Configuring the Fallback to Any {SmartProxy} Remote Execution Setting in {Project}
You can enable the *Fallback to Any {SmartProxy}* setting to configure {Project} to search for remote execution {SmartProxies} from the list of {SmartProxies} that are assigned to hosts. This can be useful if you need to run remote jobs on hosts that have no subnets configured or if the hosts' subnets are assigned to {SmartProxies} that do not have the remote execution feature enabled.

If the *Fallback to Any {SmartProxy}* setting is enabled, {Project} adds another set of {SmartProxies} to select the remote execution {SmartProxy} from. {Project} also selects the most lightly loaded {SmartProxy} from the set of all {SmartProxies} assigned to the host, such as the following:

* DHCP, DNS and TFTP {SmartProxies} assigned to the host's subnets
* DNS {SmartProxy} assigned to the host's domain
* Realm {SmartProxy} assigned to the host's realm
* Puppet Master {SmartProxy}
* Puppet CA {SmartProxy}
* OpenSCAP {SmartProxy}

.Procedure

. In the {Project} web UI, navigate to *Administer* > *Settings*.
. Click *RemoteExecution*.
. Configure the *Fallback to Any {SmartProxy}* setting.

.For CLI Users

Enter the `hammer settings set` command on {Project} to configure the *Fallback to Any {SmartProxy}* setting. For example, to set the value to `true`, enter the following command:

----
# hammer settings set --name=remote_execution_fallback_proxy --value=true
----

==== Configuring the Global {SmartProxy} Remote Execution Setting in {Project}
By default, {Project} searches for remote execution {SmartProxies} in hosts' organizations and locations regardless of whether {SmartProxies} are assigned to hosts' subnets or not. You can disable the *Enable Global {SmartProxy}* setting if you want to limit the search to the {SmartProxies} that are assigned to hosts' subnets.

If the *Enable Global {SmartProxy}* setting is enabled, {Project} adds another set of {SmartProxies} to select the remote execution {SmartProxy} from. {Project} also selects the most lightly loaded remote execution {SmartProxy} from the set of all {SmartProxies} in the host's organization and location to execute a remote job.

.Procedure

. In the {Project} web UI, navigate to *Administer* > *Settings*.
. Click *RemoteExection*.
. Configure the *Enable Global {SmartProxy}* setting.

.For CLI Users

Enter the `hammer settings set` command on {Project} to configure the `Enable Global {SmartProxy}` setting. For example, to set the value to `true`, enter the following command:

----
# hammer settings set --name=remote_execution_global_proxy --value=true
----

=== Configuring {Project} to Use an Alternative Directory to Execute Remote Jobs on Clients

By default, {Project} uses the `/var/tmp` directory on the client system to execute the remote execution jobs. If the client system has `noexec` set for the `/var/` volume or file system, you must configure {Project} to use an alternative directory because otherwise the remote execution job fails since the script cannot be run.

.Procedure

Optional: To use an alternative directory, complete this procedure.

. Create a new directory, for example _new_place_:
+
[options="nowrap", subs="+quotes,verbatim,attributes"]
----
# mkdir /_remote_working_dir_
----

. Copy the SELinux context from the default `var` directory:
+
[options="nowrap", subs="+quotes,verbatim,attributes"]
----
# chcon --reference=/var _/remote_working_dir_
----

. Edit the `remote_working_dir` setting in the `/etc/foreman-proxy/settings.d/remote_execution_ssh.yml` file to point to the required directory, for example:
+
[options="nowrap", subs="+quotes,verbatim,attributes"]
----
:remote_working_dir: _/remote_working_dir_
----

[[sect-Managing_Hosts-Establishing_a_Secure_Connection_for_Remote_Commands]]
=== Distributing SSH Keys for Remote Execution

To use SSH keys for authenticating remote execution connections, you must distribute the public SSH key from {SmartProxy} to its attached hosts that you want to manage. Ensure that the SSH service is enabled and running on the hosts. Configure any network or host-based firewalls to enable access to port 22.

Use one of the following methods to distribute the public SSH key from {SmartProxy} to target hosts:

. xref:sect-Managing_Hosts-Distributing_SSH_Keys_for_Remote_Execution_Manually[].
. xref:sect-Managing_Hosts-Using_API_to_Obtain_SSH_Keys_for_Remote_Execution[].
. xref:sect-Managing_Hosts-Configuring_a_Kickstart_Template_to_Distribute_SSH_Keys_during_Provisioning[].

{Project} distributes SSH keys for the remote execution feature to the hosts provisioned from {Project} by default.

If the hosts are running on Amazon Web Services, enable password authentication. For more information, see link:https://aws.amazon.com/premiumsupport/knowledge-center/[].

[[sect-Managing_Hosts-Distributing_SSH_Keys_for_Remote_Execution_Manually]]
==== Distributing SSH Keys for Remote Execution Manually

To distribute SSH keys manually, complete the following steps:

.Procedure

. Enter the following command on {SmartProxy}. Repeat for each target host you want to manage:
+
[options="nowrap", subs="+quotes,verbatim,attributes"]
----
# ssh-copy-id -i ~foreman-proxy/.ssh/id_rsa_foreman_proxy.pub _root@target.example.com_
----

. To confirm that the key was successfully copied to the target host, enter the following command on {SmartProxy}:
+
[options="nowrap", subs="+quotes,verbatim,attributes"]
----
# ssh -i ~foreman-proxy/.ssh/id_rsa_foreman_proxy _root@target.example.com_
----

[[sect-Managing_Hosts-Using_API_to_Obtain_SSH_Keys_for_Remote_Execution]]
==== Using the {Project} API to Obtain SSH Keys for Remote Execution

To use the {Project} API to download the public key from {SmartProxy}, complete this procedure on each target host.

.Procedure

. On the target host, create the `~/.ssh~` directory to store the SSH key:
+
----
# mkdir ~/.ssh
----

. Download the SSH key from {SmartProxy}:
+
[options="nowrap", subs="+quotes,verbatim,attributes"]
----
# curl https://_{smartproxy-example-com}_:9090/ssh/pubkey >> ~/.ssh/authorized_keys
----

. Configure permissions for the `~/.ssh` directory:
+
----
# chmod 700 ~/.ssh
----

. Configure permissions for the `authorized_keys` file:
+
----
# chmod 600 ~/.ssh/authorized_keys
----

[[sect-Managing_Hosts-Configuring_a_Kickstart_Template_to_Distribute_SSH_Keys_during_Provisioning]]
==== Configuring a Kickstart Template to Distribute SSH Keys during Provisioning

You can add a `remote_execution_ssh_keys` snippet to your custom kickstart template to deploy SSH Keys to hosts during provisioning. Kickstart templates that {Project} ships include this snippet by default. Therefore, {Project} copies the SSH key for remote execution to the systems during provisioning.

.Procedure

* To include the public key in newly-provisioned hosts, add the following snippet to the Kickstart template that you use:
+
[options="nowrap", subs="+quotes,verbatim,attributes"]
----
<%= snippet 'remote_execution_ssh_keys' %>
----

=== Configuring a keytab for Kerberos Ticket Granting Tickets

Use this procedure to configure {Project} to use a keytab to obtain Kerberos ticket granting tickets. If you do not set up a keytab, you must manually retrieve tickets.

.Procedure

To ensure that the `foreman-proxy` user on {Project} can obtain Kerberos ticket granting tickets, complete the following steps:

. Find the ID of the `foreman-proxy` user:
+
[options="nowrap", subs="+quotes,verbatim,attributes"]
----
# id -u foreman-proxy
----
+
. Modify the `umask` value so that new files have the permissions `600`:
+
[options="nowrap", subs="+quotes,verbatim,attributes"]
----
# umask 077
----
+
. Create the directory for the keytab:
+
[options="nowrap", subs="+quotes,verbatim,attributes"]
----
# mkdir -p "/var/kerberos/krb5/user/_USER_ID_"
----
+
. Create a keytab or copy an existing keytab to the directory:
+
[options="nowrap", subs="+quotes,attributes"]
----
# cp _your_client.keytab_ /var/kerberos/krb5/user/_USER_ID_/client.keytab
----
+
. Change the directory owner to the `foreman-proxy` user:
+
[options="nowrap", subs="+quotes,attributes"]
----
# chown -R foreman-proxy:foreman-proxy "/var/kerberos/krb5/user/_USER_ID_"
----
+
. Ensure that the keytab file is read-only:
+
[options="nowrap", subs="+quotes,attributes"]
----
# chmod -wx "/var/kerberos/krb5/user/_USER_ID_/client.keytab"
----
+
. Restore the SELinux context:
+
[options="nowrap", subs="+quotes,attributes"]
----
# restorecon -RvF /var/kerberos/krb5
----

=== Configuring Kerberos Authentication for Remote Execution

You can use Kerberos authentication to establish an SSH connection for remote execution on {Project} hosts.

.Prerequisites

Before you can use Kerberos authentication for remote execution on {ProjectName}, you must set up a Kerberos server for identity management and ensure that you complete the following prerequisites:

* Enroll {ProjectServer} on the Kerberos server
* Enroll the {Project} target host on the Kerberos server
* Configure and initialize a Kerberos user account for remote execution
* Ensure that the foreman-proxy user on {Project} has a valid Kerberos ticket granting ticket

.Procedure

To set up {Project} to use Kerberos authentication for remote execution on hosts, complete the following steps:

. To install and enable Kerberos authentication for remote execution, enter the following command:
+
[options="nowrap", subs="+quotes,verbatim,attributes"]
----
# {installer-scenario} \
 --foreman-proxy-plugin-remote-execution-ssh-ssh-kerberos-auth true
----
+
. To edit the default user for remote execution, in the {Project} web UI, navigate to *Administer* > *Settings* and click the *RemoteExecution* tab. In the *SSH User* row, edit the second column and add the user name for the Kerberos account.
. Navigate to *remote_execution_effective_user* and edit the second column to add the user name for the Kerberos account.

To confirm that Kerberos authentication is ready to use, run a remote job on the host.

[[sect-Managing_Hosts-Configuring_and_Running_Remote_Commands]]
=== Configuring and Running Remote Jobs

Any command that you want to apply to a remote host must be defined as a job template. After you have defined a job template you can execute it multiple times.

[[sect-Managing_Hosts-Setting_up_Job_Templates]]
==== Setting up Job Templates

{Project} provides default job templates that you can use for executing jobs. To view the list of job templates, navigate to *Hosts* > *Job templates*. If want to use a template without making changes, proceed to xref:sect-Managing_Hosts-Executing_Remote_Jobs[].

You can use default templates as a base for developing your own. Default job templates are locked for editing. Clone the template and edit the clone.

. To clone a template, in the *Actions* column, select *Clone*.

. Enter a unique name for the clone and click *Submit* to save the changes.

Job templates use the Embedded Ruby (ERB) syntax. For more information about writing templates, see the xref:appe-{Project_Link}-Managing_Hosts-Template_Writing_Reference[].

.Ansible Considerations
To create an Ansible job template, use the following procedure and instead of ERB syntax, use YAML syntax. Begin the template with `---` and to the first line, you must add `- hosts: all`. You can embed an Ansible playbook YAML file into the job template body. You can also add ERB syntax to customize your YAML Ansible template. You can also import Ansible playbooks in {Project}. For more information, see xref:Synchronizing_Templates_Repositories[].

.Parameter Variables
At run time, job templates can accept parameter variables that you define for a host. Note that only the parameters visible on the *Parameters* tab at the host's edit page can be used as input parameters for job templates.
If you do not want your Ansible job template to accept parameter variables at run time, in the {Project} web UI, navigate to *Administer* > *Settings* and click the *Ansible* tab. In the *Top level Ansible variables* row, change the *Value* parameter to *No*.

[[proc-Managing_Hosts-Creating_a_Job_Template]]
.To Create a Job Template:

. Navigate to *Hosts* > *Job templates*.
. Click *New Job Template*.
. Click the *Template* tab, and in the *Name* field, enter a unique name for your job template.
. Select *Default* to make the template available for all organizations and locations.
. Create the template directly in the template editor or upload it from a text file by clicking *Import*.
. Optional: In the *Audit Comment* field, add information about the change.

. Click the *Job* tab, and in the *Job category* field, enter your own category or select from the default categories listed in xref:tabl-Managing_Hosts-Default_Job_Template_Categories[].
. Optional: In the *Description Format* field, enter a description template. For example, `Install package %{package_name}`. You can also use `%{template_name}` and `%{job_category}` in your template.
. From the *Provider Type* list, select *SSH* for shell scripts and *Ansible* for Ansible tasks or playbooks.
. Optional: In the *Timeout to kill* field, enter a timeout value to terminate the job if it does not complete.
. Optional: Click *Add Input* to define an input parameter. Parameters are requested when executing the job and do not have to be defined in the template. For examples, see the *Help* tab.
. Optional: Click *Foreign input set* to include other templates in this job.
. Optional: In the *Effective user* area, configure a user if the command cannot use the default `remote_execution_effective_user` setting.
. Optional: If this template is a snippet to be included in other templates, click the *Type* tab and select *Snippet*.
. Click the *Location* tab and add the locations where you want to use the template.
. Click the *Organizations* tab and add the organizations where you want to use the template.
. Click *Submit* to save your changes.


You can create advanced templates by including other templates in the template syntax, see xref:sect-Managing_Hosts-Creating_Advanced_Templates[] for more information.

An advanced template is required, for example, for executing jobs that perform power actions; see xref:exam-Managing_Hosts-Including_Power_Actions_in_Templates[] for information on how to include the *Power Action - SSH Default* template in a custom template.

.For CLI Users

To create a job template using a template-definition file, enter the following command:

[options="nowrap", subs="+quotes,attributes"]
----
# hammer job-template create \
--file "_path_to_template_file_" \
--name "_template_name_" \
--provider-type SSH \
--job-category "_category_name_"
----

[[tabl-Managing_Hosts-Default_Job_Template_Categories]]

.Default Job Template Categories
[options="header"]
|====
|Job template category |Description
|Packages |Templates for performing package related actions. Install, update, and remove actions are included by default.
|Puppet |Templates for executing Puppet runs on target hosts.
|Power |Templates for performing power related actions. Restart and shutdown actions are included by default.
|Commands |Templates for executing custom commands on remote hosts.
|Services |Templates for performing service related actions. Start, stop, restart, and status actions are included by default.
|Katello |Templates for performing content related actions. These templates are used mainly from different parts of the {Project} web UI (for example bulk actions UI for content hosts), but can be used separately to perform operations such as errata installation.
|====

[[exam-Managing_Hosts-Creating_a_restorecon_Template]]
.Creating a restorecon Template
====
This example shows how to create a template called *Run Command - restorecon* that restores the default *SELinux* context for all files in the selected directory on target hosts.


. Navigate to *Hosts* > *Job templates*. Click *New Job Template*.
. Enter *Run Command - restorecon* in the *Name* field. Select *Default* to make the template available to all organizations. Add the following text to the template editor:
+
[source, Ruby]
----
restorecon -RvF <%= input("directory") %>
----
+
The `<%= input("directory") %>` string is replaced by a user-defined directory during job invocation.

. On the *Job* tab, set *Job category* to `Commands`.
. Click *Add Input* to allow job customization. Enter `directory` to the *Name* field. The input name must match the value specified in the template editor.
. Click *Required* so that the command cannot be executed without the user specified parameter.
. Select *User input* from the *Input type* list. Enter a description to be shown during job invocation, for example `Target directory for restorecon`.
. Click *Submit*.

See xref:exam-Managing_Hosts-Executing_a_restorecon_Template_on_Multiple_Hosts[] for information on how to execute a job based on this template.
====

[[sect-Managing_Hosts-Executing_Remote_Jobs]]
==== Executing Jobs

You can execute a job that is based on a job template against one or more hosts.

[[proc-Managing_Hosts-Executing_a_Remote_Job]]
.Procedure

. Navigate to *Hosts* > *All Hosts* and select the target hosts on which you want to execute a remote job. You can use the search field to filter the host list.
. From the *Select Action* list, select *Schedule Remote Job*.
. On the *Job invocation* page, define the main job settings:
.. Select the *Job category* and the *Job template* you want to use.
.. Optional: Select a stored search string in the *Bookmark* list to specify the target hosts.
.. Optional: Further limit the targeted hosts by entering a *Search query*. The *Resolves to* line displays the number of hosts affected by your query. Use the refresh button to recalculate the number after changing the query. The preview icon lists the targeted hosts.
.. The remaining settings depend on the selected job template. See xref:proc-Managing_Hosts-Creating_a_Job_Template[] for information on adding custom parameters to a template.
. Optional: To configure advanced settings for the job, click *Display advanced fields*. Some of the advanced settings depend on the job template, the following settings are general:

* *Effective user* defines the user for executing the job, by default it is the SSH user.

* *Concurrency level* defines the maximum number of jobs executed at once, which can prevent overload of systems' resources in a case of executing the job on a large number of hosts.

* *Timeout to kill* defines time interval in seconds after which the job should be killed, if it is not finished already. A task which could not be started during the defined interval, for example, if the previous task took too long to finish, is canceled.

* *Type of query* defines when the search query is evaluated. This helps to keep the query up to date for scheduled tasks.

* *Execution ordering* determines the order in which the job is executed on hosts: alphabetical or randomized.
+
*Concurrency level* and *Timeout to kill* settings enable you to tailor job execution to fit your infrastructure hardware and needs.

. To run the job immediately, ensure that *Schedule* is set to *Execute now*. You can also define a one-time future job, or set up a recurring job. For recurring tasks, you can define start and end dates, number and frequency of runs. You can also use cron syntax to define repetition. For more information about cron, see the https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/ch-automating_system_tasks[Automating System Tasks] section of the Red Hat Enterprise Linux 7 _System Administrator's Guide_.

. Click *Submit*. This displays the *Job Overview* page, and when the job completes, also displays the status of the job.

.For CLI Users

Enter the following command on {Project}:
+
----
# hammer settings set --name=remote_execution_global_proxy --value=false
----

To execute a remote job with custom parameters, complete the following steps:

. Find the ID of the job template you want to use:
+
----
# hammer job-template list
----

. Show the template details to see parameters required by your template:
+
[options="nowrap", subs="+quotes,attributes"]
----
# hammer job-template info --id _template_ID_
----

. Execute a remote job with custom parameters:
+
[options="nowrap", subs="+quotes,attributes"]
----
# hammer job-invocation create \
--job-template "_template_name_" \
--inputs _key1_="_value_",_key2_="_value_",... \
--search-query "_query_"
----
+
Replace _query_ with the filter expression that defines hosts, for example `"name ~ rex01"`. For more information about executing remote commands with hammer, enter `hammer job-template --help` and `hammer job-invocation --help`.

[[exam-Managing_Hosts-Executing_a_restorecon_Template_on_Multiple_Hosts]]
.Executing a restorecon Template on Multiple Hosts
====
This example shows how to run a job based on the template created in xref:exam-Managing_Hosts-Creating_a_restorecon_Template[] on multiple hosts. The job restores the SELinux context in all files under the */home/* directory.

. Navigate to *Hosts* > *All hosts* and select target hosts. Select *Schedule Remote Job* from the *Select Action* list.
. In the *Job invocation* page, select the `Commands` job category and the `Run Command - restorecon` job template.
. Type `/home` in the *directory* field.
. Set *Schedule* to `Execute now`.
. Click *Submit*. You are taken to the *Job invocation* page where you can monitor the status of job execution.

====
[[sect-Managing_Hosts-Monitoring_Remote_Jobs]]
==== Monitoring Jobs

You can monitor the progress of the job while it is running. This can help in any troubleshooting that may be required.

Ansible jobs run on batches of 100 hosts, so you cannot cancel a job running on a specific host. A job completes only after the Ansible playbook runs on all hosts in the batch.

[[proc-Managing_Hosts-Monitoring_a_Remote_Job]]
.To Monitor a Job:

. Navigate to the Job page. This page is automatically displayed if you triggered the job with the `Execute now` setting. To monitor scheduled jobs, navigate to *Monitor* > *Jobs* and select the job run you wish to inspect.
. On the Job page, click the *Hosts* tab. This displays the list of hosts on which the job is running.
. In the *Host* column, click the name of the host that you want to inspect. This displays the *Detail of Commands* page where you can monitor the job execution in real time.
. Click *Back to Job* at any time to return to the *Job Details* page.

.For CLI Users

To monitor the progress of a job while it is running, complete the following steps:

. Find the ID of a job:
+
[options="nowrap", subs="+quotes,attributes"]
----
# hammer job-invocation list
----

. Monitor the job output:
+
[options="nowrap", subs="+quotes,attributes"]
----
# hammer job-invocation output \
--id _job_ID_ \
--host _host_name_
----

. Optional: to cancel a job, enter the following command:
+
[options="nowrap", subs="+quotes,attributes"]
----
# hammer job-invocation cancel \
--id _job_ID_
----

[[sect-Managing_Hosts-Creating_Advanced_Templates]]
==== Creating Advanced Templates

When creating a job template, you can include an existing template in the template editor field. This way you can combine templates, or create more specific templates from the general ones.

The following template combines default templates to install and start the *httpd* service on Red Hat Enterprise Linux systems:

[source, Ruby]
----
<%= render_template 'Package Action - SSH Default', :action => 'install', :package => 'httpd' %>
<%= render_template 'Service Action - SSH Default', :action => 'start', :service_name => 'httpd' %>
----

The above template specifies parameter values for the rendered template directly. It is also possible to use the *input()* method to allow users to define input for the rendered template on job execution. For example, you can use the following syntax:

[source, Ruby]
----
<%= render_template 'Package Action - SSH Default', :action => 'install', :package => input("package") %>
----

With the above template, you have to import the parameter definition from the rendered template. To do so, navigate to the *Jobs* tab, click *Add Foreign Input Set*, and select the rendered template from the *Target template* list. You can import all parameters or specify a comma separated list.

[[exam-Managing_Hosts-Rendering_a_restorecon_Template]]
.Rendering a restorecon Template
====
This example shows how to create a template derived from the *Run command - restorecon* template created in xref:exam-Managing_Hosts-Creating_a_restorecon_Template[]. This template does not require user input on job execution, it will restore the SELinux context in all files under the */home/* directory on target hosts.

Create a new template as described in xref:sect-Managing_Hosts-Setting_up_Job_Templates[], and specify the following string in the template editor:
[source, Ruby]
----
<%= render_template("Run Command - restorecon", :directory => "/home") %>
----

====

[[exam-Managing_Hosts-Including_Power_Actions_in_Templates]]
.Including Power Actions in Templates
====
This example shows how to set up a job template for performing power actions, such as reboot. This procedure prevents {Project} from interpreting the disconnect exception upon reboot as an error, and consequently, remote execution of the job works correctly.

Create a new template as described in xref:sect-Managing_Hosts-Setting_up_Job_Templates[], and specify the following string in the template editor:
[source, Ruby]
----
<%= render_template("Power Action - SSH Default", :action => "restart") %>
----

====

[[sect-Managing_Hosts-Delegating_Permissions]]
=== Delegating Permissions for Remote Execution

You can control which users can run which jobs within your infrastructure, including which hosts they can target. The remote execution feature provides two built-in roles:

* *Remote Execution Manager*: This role allows access to all remote execution features and functionality.

* *Remote Execution User*: This role only allows running jobs; it does not provide permission to modify job templates.



You can clone the Remote Execution User role and customize its filter for increased granularity. If you adjust the filter with the `view_job_templates` permission, the user can only see and trigger jobs based on matching job templates. You can use the `view_hosts` and `view_smart_proxies` permissions to limit which hosts or {SmartProxies} are visible to the role.

The `execute_template_invocation` permission is a special permission that is checked immediately before execution of a job begins. This permission defines which job template you can run on a particular host. This allows for even more granularity when specifying permissions. For more information on working with roles and permissions see link:{BaseURL}administering_red_hat_satellite/chap-red_hat_satellite-administering_red_hat_satellite-users_and_roles#sect-{Project_Link}-Administering_{Project_Link}-Users_and_Roles-Creating_and_Managing_Roles[Creating and Managing Roles] in the _Administering {ProjectName}_.

The following example shows filters for the `execute_template_invocation` permission:

[options="nowrap", subs="+quotes,verbatim,attributes"]
----
name = Reboot and host.name = staging.example.com
name = Reboot and host.name ~ *.staging.example.com
name = "Restart service" and host_group.name = webservers
----
The first line in the above example permits the user to apply the *Reboot* template to one selected host. The second line defines a pool of hosts with names ending with *.staging.example.com*. The third line binds the template with a host group.

[NOTE]
====
Permissions assigned to users can change over time. If a user has already scheduled some jobs to run in the future, and the permissions have changed, this can result in execution failure because the permissions are checked immediately before job execution.
====
