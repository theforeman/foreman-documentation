[id="preparing-for-disaster-recovery-with-active-and-passive-project-server-with-external-storage"]
= Preparing for disaster recovery with active and passive {ProjectServer} with external storage

Create your passive {ProjectServer} as a clone of your active {ProjectServer}.
Make sure that both servers connect to the database content on your network-attached storage (NAS).

.Procedure
. Replicate the `/var/lib/pulp` and `{postgresql-lib-dir}` directories from the active {ProjectServer} to your NAS.
// Do we want to keep this generic like the virtualization scenario, or is this a good place to provide example rsync commands or something?
. Clone your active {ProjectServer}.
See xref:cloning_satellite_server[].
+
Keep the source server powered on and power off the new cloned server.
The source server remains your primary active server, while the clone server becomes the secondary passive server.
. Determine how you want to attach the database content on the NAS to your passive server:
* If you mount the storage directly by both your active and passive server, the servers will always see the same, up-to-date content.
* If you mount the storage only to your active server and replicate the storage periodically to another location, the passive server will access the data only if it takes over as the active server.
// This would be a good place to talk more about the pros and cons of these two alternatives, wouldn't it? What would be those pros and cons?

.Verification
Perform this test in an isolated staging environment:
// Does this make sense? We don't want users testing this in production, right?

. Stop the active server to mimic failure.
// How? foreman-maintain service stop?
. Start the passive server.
// How? foreman-maintain service start?
. Verify that your passive server can access the data stored on your NAS.
. Assess the functionality of the test {ProjectServer}.
See xref:Retrieving_the_Status_of_Services_{context}[].
. Perform these verification checks regularly.
