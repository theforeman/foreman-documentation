[id="Adding_Proxmox_Details_to_a_Compute_Profile_{context}"]
= Adding Proxmox Details to a Compute Profile

You can predefine certain hardware settings for virtual machines on Proxmox.
You achieve this through adding these hardware settings to a compute profile.

.Procedure
. In the {ProjectWebUI}, navigate to *Infrastructure* > *Compute Profiles*.
. Select a compute profile.
. Select a Proxmox compute resource.
. In the *CPUs* field, enter the number of CPUs to allocate to the host.

// FIXME: WIP
* In the *Type* list, select the kind of virtualization solution hosts will run on when created using this compute profile.
In Proxmox, you may choose between _LXC_ (Linux containers) and _KVM/QEMU_ (kernel-based virtual machines).
LXC focuses more on isolating applications and reusing the host kernel, whereas KVM lets you boot various guest operating systems and therefor runs its own kernel.
* The *VM ID* reset unique number by Proxmox to identify its guest machines.
* The *Node* drop down menu lets you choose on which node the compute profile runs on.
This mostly makes sense if you run a Proxmox cluster.
* The *Image* drop down menu lists available images on the Proxmox hypervisor.
Select one to choose on which the compute profile is to be run on.
* The *Main options*, *CPU*, *Memory*, *CDROM*, and *OS* checkboxes can be optionally ticked to gain visual access to configure the compute profile in more detail.
Configuring the _network interfaces_ and _storage_ is mandatory.
* You may give the compute profile an arbitrary *Description*.
* In the *Boot Device Order* field you may choose a custom root device order.
The default is trying to boot from _hard disk_ first, _CDROM_ second, and _network_ third.
* Checking the *Start at Boot* checkbox lets you automatically boot up your newly created host running the selected compute profile.
This can be overwritten for individual hosts on the create new host page.
* Checking the *Qemu Agent* checkbox automatically installs a QEMU agent on the guest system.
This is useful to properly shutdown guest machines using ACPI commands.
* The *KVM* lets you enable or disable KVM based hardware virtualization.
* You may choose a specific keyboard layout in the *Keyboard* drop down menu.
* The *VGA* checkbox allows you to select the default output type.
* The *Type* drop down menu allows you to select an architecture different than the architecture Proxmox is running on.
This is based on QEMU's hardware emulation capabilities.
* The *Sockets* field allows you to set the number of CPU sockets the compute profile will run on.
This is mostly done for software licensing reasons, that is assign 16 cores to a virtual machines which consists of four quad core CPUs.
* The *Cores* field lets you assign multiple cores to hosts based on the selected compute profile.
* The *VCPUs* field allows you to assign virtual CPUs (that is simultaneous multithreading) to the host.
The default value is the product of _sockets_ and _cores_.
* The *CPU limit* field allows you to set the maximum number of CPU cores the guest system is allowed to use.
Using _virtual CPUs_ instead of _cores_ lets you hide information from the guest system.
This limits the guest system to a specified maximum CPU time.
* The *CPU units* field lets you specify the relative CPU time managed by the _kernel fair scheduler_.
The default value is 1000 and it only accepts positive numbers larger than 8.
* You may choose to enable non uniform memory access with checking the *Enable NUMA* checkbox.
This lets you optimize access time to memory of CPUs.
* The *PCID* checkbox lets you mitigate performance impacts of meltdown patches to secure kernel page table isolation.
* Ticking the *Spectre-CTRL* checkbox enables Spectre v1 and v2 mitigations on Intel CPUs.
* The *Memory* field lets you specify a maximum amount of memory available to the Proxmox guest.
* The *Minimum memory* field lets you specify a minimum amount of memory available to the Proxmox guest.
* The *Shares* field specifies the weight of the host using this compute profile.
Proxmox uses the shares number to adjust resources accordingly.
* Activating the *Ballooning Device* checkbox lets you optionally add more memory after creating a host.
* By default, there is no *CDROM* selected.
Optionally, you may select *Image* and specify a _storage path_ and _image ISO_ name as CDROM.
* On the *OS Type* drop down menu you may select the appropriate operating system type for the host to be created with the selected compute profile.
* If there are no network capabilities required, you could choose to remove the default NIC by clicking the red cross.
This is not recommended.
* The *Card* drop down menu lets you select a network interface card present on the Proxmox machine.
* The *Bridge* drop down menu lets you select which bridge the to be created host will be assigned to.
* The *VLAN tag* field lets you input the corresponding VLAN tag which your Proxmox machine is part of.
* The *Rate Limit* lets you limit networking traffic.
* The *Multiqueue* options allow hosts to use multiple virtual CPUs to process network packets.
* The *Firewall* checkbox allows you to protect the interface with a firewall.
* The *Disconnect* checkbox lets you disable the network interface card by default when starting the machine.
* Optionally, you may add another network interface by clicking the *Add Interface* button.
This presents the same options as shown above (that is option 30 to 37).
* You may delete the default disk by clicking the red cross.
This is not recommended.
* In the *Storage* drop down menu lets you select the storage pool available to the Proxmox compute resource.
* In the *Controller* drop down menu lets you select an appropriate storage controller.
It is recommended to use either _SCSI_ or _VirtIO Block_.
* The *Device* field needs to be manually iterated starting with zero.
If you choose to add a second storage device, it would get _1_ as its device number.
* Optional: In the *Cache* drop down menu, activate caching for the selected disk.
The use of caching is not recommended.
* In the *Size* field, specify the size of the hard disk available to hosts using this compute profile.
* Optional: Click *Add Volume* to add more storage devices.

. In the *Cores per socket* field, enter the number of cores to allocate to each CPU.
. In the *Memory* field, enter the amount of memory in MiB to allocate to the host.
. In the *Firmware* checkbox, select either _BIOS_ or _UEFI_ as firmware for the host.
By default, this is set to _automatic_.
. In the *Cluster* list, select the name of the target host cluster on the Proxmox environment.
. From the *Resource pool* list, select an available resource allocations for the host.
. In the *Folder* list, select the folder to organize the host.
. From the *Guest OS* list, select the operating system you want to use in Proxmox.
. From the *Virtual H/W version* list, select the underlying Proxmox hardware abstraction to use for virtual machines.
. If you want to add more memory while the virtual machine is powered on, select the *Memory hot add* check box.
. If you want to add more CPUs while the virtual machine is powered on, select the *CPU hot add* check box.
. If you want to add a CD-ROM drive, select the *CD-ROM drive* check box.
. From the *Boot order* list, define the order in which the virtual machines tried to boot.
. Optional: In the *Annotation Notes* field, enter an arbitrary description.
. If you use image-based provisioning, select the image from the *Image* list.
. From the *SCSI controller* list, select the disk access method for the host.
. If you want to use eager zero thick provisioning, select the *Eager zero* check box.
By default, the disk uses lazy zero thick provisioning.
. From the *Network Interfaces* list, select the network parameters for the host's network interface.
At least one interface must point to a {SmartProxy}-managed network.
. Optional: Click *Add Interface* to create another network interfaces.

. Click *Submit* to save the compute profile.
