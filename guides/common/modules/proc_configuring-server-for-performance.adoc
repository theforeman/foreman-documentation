[id="Configuring_Server_for_Performance_{context}"]
= Configuring {Project} for Performance

{ProjectName} as a product comes with a number of components that communicate with each other to produce a final outcome.
All these components can be tuned independently of each other to achieve the maximum possible performance for the scenario desired.

== Tuned profile

{RHEL} 7 enables the tuned daemon by default during installation.
On bare-metal, it is recommended that {ProjectName} and capsule servers run the `throughput-performance` tuned profile.
While, if virtualized, they should run the `virtual-guest` profile.
If it is not certain the system is currently running the correct profile, check with the `tuned-adm active` command as shown above.
More information about tuned is located in the {RHEL} Performance Tuning Guide:

----
# service tuned start
# chkconfig tuned on
RHEL 7 (bare-metal):
# tuned-adm profile throughput-performance
RHEL 7 (virtual machine)
# tuned-adm profile virtual-guest
----

Transparent Huge Pages is a memory management technique used by the Linux kernel which reduces the overhead of using Translation Lookaside Buffer (TLB) by using larger sized memory pages.
Due to databases having Sparse Memory Access patterns instead of Contiguous Memory access patterns, database workloads often perform poorly when Transparent Huge Pages is enabled.
To improve performance of PostgreSQL, {Team} recommends Transparent Huge Pages be disabled.
In deployments where the PostgreSQL database is running on a separate server, there may be a small benefit to using Transparent Huge Pages on the {ProjectServer} only.
For details on disabling Transparent Huge Pages, see https://access.redhat.com/solutions/1320153[Red Hat Solution].

== Apache HTTPD Performance Tuning

Apache httpd forms a core part of the {Project} and acts as a web server for handling the requests that are being made through the {ProjectWebUI} or exposed APIs.
To increase the concurrency of the operations, httpd forms the first point where tuning can help to boost the performance of your {Project}.

=== Configuring how many processes can be launched by Apache httpd

The version of Apache httpd that ships with {ProjectName} by default uses prefork request handling mechanism.
With the prefork model of handling the requests, httpd will launch a new process to handle the incoming connection by the client.

When the number of requests to the apache exceed the maximum number of child processes that can be launched to handle the incoming connections, an HTTP 503 Service Unavailable Error is raised by Apache.

Amidst httpd running out of processes to handle the incoming connections can also result in multiple component failure on your {Project} side due to the dependency of components like Pulp on the availability of httpd processes.

Based on your expected peak load, you might want to modify the configuration of apache prefork to enable it to handle more concurrent requests.

An example modification to the prefork configuration for a server which may desire to handle 150 concurrent content host registrations to Satellite may look like the configuration file example that follows (see how to use `custom-hiera.yaml` file; this will modify config file `/etc/httpd/conf.modules.d/prefork.conf`):

----
File: /etc/foreman-installer/custom-hiera.yaml
apache::mod::prefork::serverlimit: 582
apache::mod::prefork::maxclients: 582
apache::mod::prefork::startservers: 10
----

In the above example, the ServerLimit parameter is set only to be able to raise MaxClients value.

The MaxClients (see MaxRequestWorker which is a new name in Apache docs) parameter is being used to set the maximum number of child processes that httpd can launch to handle the incoming requests.

The StartServers parameter defines how many server processes will be launched by default when the httpd process is started.

=== Increasing the MaxOpenFiles Limit

With the tuning in place, apache httpd can easily open a lot of file descriptors on the server which may exceed the default limit of most of the linux systems in place.
To avoid any kind of issues that may arise as a result of exceeding max open files limit on the system, please create the following file and directory and set the contents of the file as specified in the below given example:

----
File: /etc/systemd/system/httpd.service.d/limits.conf
[Service]
LimitNOFILE=640000
----

Once the file has been edited, the following commands need to be run to make the tunings come into effect:

----
# systemctl daemon-reload
# foreman-maintain service restart
----

== qdrouterd and qpid tunings

=== Calculating the maximum open files limit for qdrouterd

Calculate the limit for open files in qdrouterd using this formula: [.title-ref]#(N x 3) + 100#, where N is the number of content hosts.
Each content host may consume up to three file descriptors in the router, and 100 filedescriptors are required to run the router itself.

The following settings permit Satellite to scale up to 10,000 content hosts.

Add/Update [.title-ref]#qpid::router::open_file_limit# in [.title-ref]#/etc/foreman-installer/custom-hiera.yaml# as shown below:

----
qpid::router::open_file_limit: 150100
----

Note The change must be applied via:

----
# {foreman-installer}
# systemctl daemon-reload
# {foreman-maintain} service restart
----

=== Calculating the maximum open files limit for qpidd

Calculate the limit for open files in qpidd using this formula: [.title-ref]#(N x 4) + 500#, where N is the number of content hosts.
A single content host can consume up to four file descriptors and 500 file descriptors are required for the operations of Broker (a component of qpidd).

Add/Update [.title-ref]#qpid::open_file_limit# in [.title-ref]#/etc/foreman-installer/custom-hiera.yaml# as shown below:

----
qpid::open_file_limit: 65536
----

Note The change must be applied via:

----
# {foreman-installer}
# systemctl daemon-reload
# {foreman-maintain} service restart
----

=== Maximum asynchronous input-output (AIO) requests

Increase the maximum number of allowable concurrent AIO requests by increasing the kernel parameter [.title-ref]#fs.aio-max-nr#.

Edit configuration file [.title-ref]#/etc/sysctl.conf#, setting the value of [.title-ref]#fs.aio-max-nr# to the desired maximum.

----
fs.aio-max-nr=23456
----

In this example, 23456 is the maximum number of allowable concurrent AIO requests.

This number should be bigger than 33 multiplied by the maximum number of the content hosts planned to be registered to Satellite.
To apply the changes:

----
# sysctl -p
----

Rebooting the machine also ensures that this change is applied.

=== Storage Considerations

Plan to have enough storage capacity for directory [.title-ref]#/var/lib/qpidd# in advance when you are planning an installation that will use katello-agent extensively.
In {ProjectName}, [.title-ref]#/var/lib/qpidd# requires 2MB disk space per content host.
See this https://bugzilla.redhat.com/show_bug.cgi?id=1366323[bug] for more details.

=== mgmt-pub-interval setting

You might see the following error in journal (use `journalctl` command to access it) in {RHEL} 7:

----
{foreman-example-com} qpidd[92464]: [Broker] error Channel exception: not-attached: Channel 2 is not attached(/builddir/build/BUILD/qpid-cpp-0.30/src/qpid/amqp_0_10/SessionHandler.cpp: 39
{foreman-example-com} qpidd[92464]: [Protocol] error Connectionqpid.10.1.10.1:5671-10.1.10.1:53790 timed out: closing
----

This error message appears because qpid maintains management objects for queues, sessions, and connections and recycles them every ten seconds by default.
The same object with the same ID is created, deleted, and created again.
The old management object is not yet purged, which is why qpid throws this error.
Here’s a workaround: lower the mgmt-pub-interval parameter from the default 10seconds to something lower.
Add it to /etc/qpid/qpidd.conf and restart the qpidd service.
See also https://bugzilla.redhat.com/show_bug.cgi?id=1335694[Bug 1335694] comment 7.

== Puma Tunings

Puma is a ruby application server which is used for serving the Foreman related requests to the clients.

For any Satellite configuration that is supposed to handle a large number of clients or frequent operations, it is important for the Puma to be tuned appropriately.

=== Threads min effects

Less threads will lead to more memory usage for different scales on your {ProjectServer}.

For example, we have compared these two setups:

[width="100%",cols="50%,50%",options="header",]
|===
|{Project} VM with 8 CPUs, 40 GB RAM |{Project} VM with 8 CPUs, 40 GB RAM
|--foreman-foreman-service-puma-threads-min=0 |--foreman-foreman-service-puma-threads-min=16
|--foreman-foreman-service-puma-threads-max=16 |--foreman-foreman-service-puma-threads-max=16
|--foreman-foreman-service-puma-workers=2 |--foreman-foreman-service-puma-workers=2
|===

When we tune the puma server with t_min=16 puma will consume about 12% less memory as compared to t_min=0.

=== Setting threads min, max & workers

More workers will allow for lower time to register hosts in parallel.

For example, we have compared these two setups:

[width="100%",cols="50%,50%",options="header",]
|===
|Satellite VM with 8 CPUs, 40 GB RAM |Satellite VM with 8 CPUs, 40 GB RAM
|--foreman-foreman-service-puma-threads-min=16 |--foreman-foreman-service-puma-threads-min=8
|--foreman-foreman-service-puma-threads-max=16 |--foreman-foreman-service-puma-threads-max=8
|--foreman-foreman-service-puma-workers=2 |--foreman-foreman-service-puma-workers=4
|===

In the second case with more workers but the same total number of threads, we have seen about 11% of speedup in highly concurrent registrations scenario.
Moreover, adding more workers did not consume more cpu and memory but will get more performance.

=== Setting right number of workers for different number of CPUs

If you have enough CPUs, adding more workers adds more performance.

For example, we have compared Satellite setups with 8 and 16 CPUs.

[width="100%",cols="50%,50%",options="header",]
|===
|Satellite VM with 8 CPUs, 40 GB RAM |Satellite VM with 16 CPUs, 40 GB RAM
|--foreman-foreman-service-puma-threads-min=16 |--foreman-foreman-service-puma-threads-min=16
|--foreman-foreman-service-puma-threads-max=16 |--foreman-foreman-service-puma-threads-max=16
|--foreman-foreman-service-puma-workers=2,4,8 and 16 |--foreman-foreman-service-puma-workers=2,4,8 and 16
|===

In 8 CPUs setup, changing the number of workers from 2 to 16, improved concurrent registration time by 36%.
In 16 CPU setup, the same change caused 55% improvement.

Adding more workers can also help with total registration concurrency Satellite can handle.
In our measurements, setups with 2 workers were able to handle up to 480 concurrent registrations, but adding more workers improved the situation.

=== Installer auto-tuning

If the user does not provide any Puma workers and thread values via installer command line (or they are not present in the Satellite configuration), the installer tries to do its best to configure a balanced number of workers.
It follows this formula:

....
min(CPU*1.5, RAM_IN_GB - 1.5)
....

which is too much wrt. memory - there have been cases where too many workers triggered OOM on Satellite.

This should be fine for most cases, but with some usage patterns tuning is needed to either limit the amount of resources dedicated to Puma (so other Satellite components can use these) or for any other reason.
Each Puma worker consumes around 1 GB of RAM.

For your current setting see this:

----
# cat /etc/systemd/system/foreman.service.d/installer.conf
[Service]
User=foreman
Environment=FOREMAN_ENV=production
Environment=FOREMAN_HOME=/usr/share/foreman
Environment=FOREMAN_PUMA_THREADS_MIN=5
Environment=FOREMAN_PUMA_THREADS_MAX=5
Environment=FOREMAN_PUMA_WORKERS=30
# pgrep -u foreman --list-full | grep 'puma: cluster worker'
3466 puma: cluster worker 0: 3385 [foreman]
3471 puma: cluster worker 1: 3385 [foreman]
3477 puma: cluster worker 2: 3385 [foreman]
[...]
----

=== Recommendations

In order to recommend thread and worker configurations for the different tuning profiles, we conducted Puma tuning testing on Satellite 6.10 with different tuning profiles and the main test run performed in this testing was concurrent registration with the following combinations along with different workers and threads.

As of now our recommendation is based purely on concurrent registration performance, so it might not reflect your exact use-case (for example if your setup of very content oriented with lots of publishes and promotes, you might want to limit resources consumed by Puma in favor of Pulp and PostgreSQL):

[width="100%",cols="16%,19%,7%,7%,31%,20%",options="header",]
|===
|Name |Number of managed host |RAM |Cores |Recommended Puma Threads for both min & max |Recommended Puma Workers
|default |0-5000 |20GiB |4 |16 |4-6
|medium |5000-10000 |32GiB |8 |16 |8-12
|large |10000-20000 |64GiB |16|16 |12-18
|extra-large |20000-60000 |128GiB |32 |16 |16-24
|extra-extra-large |60000+ |256GiB+ |48+ |16 |20-26
|===

=== foreman-db-pool

The effective value of $db_pool will be automatically set to equal $foreman::foreman_service_puma_threads_max.
It will be the maximum of $foreman::db_pool and $foreman::foreman_service_puma_threads_max but both have default value 5, so any increase to the max threads above 5 will automatically increase the database connection pool by the same amount

For details of how that is implemented, https://github.com/theforeman/puppet-foreman/commit/026d47434316b8ae318c5e42936edc12859ab475[check set DB pool size dynamically.]

Reasoning behind these numbers:

Use 16 threads with all the tuning profiles - we have seen up to 23% performance increase with 16 threads when compared to 5 threads (14% for 8 compared to 4 and 10% for 32 compared to 4) - see table below:

[width="100%",cols="17%,21%,20%,21%,21%",options="header",]
|===
| |4 workers, 4 threads |4 workers, 8 threads |4 workers, 16 threads |4 workers, 32 threads
|Improvement| 0%| 14%| 23%| 10%
|===

Use 4 - 6 workers on a default setup (4 CPUs) - we have seen about 25% higher performance with 5 workers when compared to 2 workers, but 8% lower performance with 8 workers when compared to 2 workers - see table below:

[width="100%",cols="17%,21%,20%,21%,21%",options="header",]
|===
| |2 workers, 16 threads |4 workers, 16 threads |6 workers, 16 threads |8 workers, 16 threads
|Improvement |0% |26% |22% |-8%
|===

Use 8 - 12 workers on a medium setup (8 CPUs) - see table below:

[width="100%",cols="16%,17%,16%,17%,17%,17%",options="header",]
|===
| |2 workers, 16 threads |4 workers, 16 threads |8 workers, 16 threads |12 workers, 16 threads |16 workers, 16 threads
|Improvement |0% |51% |52% |52% |42%
|===

Use 16 - 24 workers on a 32 CPUs setup (this was tested on a 90 GB RAM machine and memory turned out to be a factor here as system started swapping - proper “extra-large” should have 128GB), higher number of workers was problematic for higher registration concurrency levels we tested, so we can not recommend it.

[width="100%",cols="13%,14%,14%,14%,15%,15%,15%",options="header",]
|===
| |4 workers, 16 threads |8 workers, 16 threads |16 workers, 16 threads |24 workers, 16 threads |32 workers, 16 threads |48 workers, 16 threads
|Improvement |0% |37% |44% |52% |too many failures |too many failures
|===

== Dynflow Tuning

Dynflow is the workflow management system and task orchestrator which is built as a plugin inside Foreman and is used to execute the different tasks of Satellite in an out-of-order execution manner.
Under the conditions when there are a lot of clients checking in on Satellite and running a number of tasks, the Dynflow can take some help from an added tuning specifying how many executors can it launch.

The following configuration snippet provides more information about the tunings involved related to Dynflow: https://satellite.example.com/foreman_tasks/sidekiq

== PostgreSQL Tuning

PostgreSQL is the primary SQL based database that is used by Satellite for the storage of persistent context across a wide variety of tasks that Satellite does.
The database sees an extensive usage is usually working on to provide the Satellite with the data which it needs for its smooth functioning.
This makes PostgreSQL a heavily used process which if tuned can have a number of benefits on the overall operational response of Satellite.

The below set of tunings can be applied to PostgreSQL to improve its response times (see [.title-ref]#how to use custom-hiera.yaml# file; this will modify [.title-ref]#/var/lib/pgsql/data/postgresql.conf# file):

----
File: /etc/foreman-installer/custom-hiera.yaml
postgresql::server::config_entries:
  max_connections: 1000
  shared_buffers: 2GB
  work_mem: 8MB
  autovacuum_vacuum_cost_limit: 2000
----

In the above tuning configuration, there are a certain set of keys which we have altered:

`max_connections`: The key defines the maximum number of connections that can be accepted by the PostgreSQL processes that are running.
An optimal value for the parameter will be equal to the nearest multiple of 100 of the ServerLimit value of Apache httpd2 multiplied by 2.
For example, if ServerLimit is set to 582, we can set the max_connections to 1000.

`shared_buffers`: The shared buffers define the memory used by all the active connections inside postgresql to store the data for the different database operations.
An optimal value for this will vary between 2 GB to a maximum of 25% of your total system memory depending upon the frequency of the operations being conducted on Satellite.

`work_mem`: The work_mem is the memory that is allocated on per process basis for Postgresql and is used to store the intermediate results of the operations that are being performed by the process.
Setting this value to 8 MB should be more than enough for most of the intensive operations on Satellite.

`autovacuum_vacuum_cost_limit`: The key defines the cost limit value for the vacuuming operation inside the autovacuum process to clean up the dead tuples inside the database relations.
The cost limit defines the number of tuples that can be processed in a single run by the process.
An optimal value for this is 2000 based on the general load that Satellite pushes on the PostgreSQL server process.

Note - With the upgrade to Postgres 12, ‘checkpoint_segments’ configuration is not supported.
For more details, please refer to this https://bugzilla.redhat.com/show_bug.cgi?id=1867311#c12[bugzilla] .

=== Benchmarking raw DB performance

To get a list of the top table sizes in disk space for both Candlepin and Foreman, check https://github.com/RedHatSatellite/satellite-support/blob/master/postgres-size-report[postgres-size-report] script in https://github.com/RedHatSatellite/satellite-support[satellite-support] git repository.

PGbench utility (note you may need to resize PostgreSQL data directory /var/lib/pgsql/ directory to 100GB or what does benchmark take to run) might be used to measure PostgreSQL performance on your system.
Use yum install postgresql-contrib to install it.
Some resources are:
See https://github.com/RedHatSatellite/satellite-support

Choice of filesystem for PostgreSQL data directory might matter as well.
See https://blog.pgaddict.com/posts/postgresql-performance-on-ext4-and-xfs

[NOTE]
====
* Never do any testing on production system and without valid backup.
* Before you start testing, see how big the database files are.
Testing with a really small database would not produce any meaningful results.
For example, if the DB is only 20G and the buffer pool is 32G, it won't show problems with large number of connections because the data will be completely buffered.
====

== {SmartProxy} Configuration Tuning

Capsules (called Smart Proxies in upstream Foreman) are meant to offload part of Satellite load related to distributing content to clients but they can also be used to execute Remote Execution jobs.
What they can not help with is anything which extensively uses Satellite API as host registration or package profile update.

=== Initial results

As of now testing for Capsule tuning recommendations is ongoing, but we are sharing some initial results here already.
We have measured multiple test cases on multiple Capsule 6.10 configurations:

[width="79%",cols="48%,19%,33%",options="header",]
|===
|Capsule HW configuration |CPUs |memory
|minimal |4 |12 GiB
|large |8 |24 GiB
|extra large |16 |46 GiB
|===

For concurrent registrations a bottleneck is CPU speed, but all configs were able to handle even high concurrency without swapping.

We have tested executing Remote Execution jobs via both SSH and Ansible backend on 500, 2000 and 4000 hosts.
All configurations were able to handle all of the tests without errors, except for the smallest configuration (4CPUs and 12 GB memory) which failed to finish on all 4000 hosts.

In a sync test where we synced {RHEL} 6, 7, 8 BaseOS and 8 AppStream we have not seen significant differences amongst {SmartProxy} configurations.
This will be different for syncing a higher number of content views in parallel.
