[[chap-Red_Hat_Satellite-Architecture_Guide-Deployment_Considerations]]
== Deployment Considerations

This section provides an overview of general topics to be considered when planning a {ProjectName} deployment together with recommendations and references to more specific documentation.
ifeval::["{build}" == "satellite"]
For an example implementation based on a sample customer scenario (specific to {ProjectXY}), see the Red{nbsp}Hat Knowledgebase solution https://access.redhat.com/articles/1585273[10 Steps to Build an SOE: How Red Hat Satellite 6 Supports Setting up a Standard Operating Environment].
endif::[]

ifeval::["{build}" == "satellite"]
[[sect-Satellite_Server_Configuration]]
=== {ProjectServer} Configuration

The first step to a working {Project} infrastructure is installing an instance of {ProjectName} Server on a dedicated {RHEL} 7 Server.

* For more information about installing {ProjectServer} from a connected network, see {InstallingProjectDocURL}[_{project-installation-guide-title}_] and {InstallingProjectDocURL}preparing-environment-for-installation[Preparing your Environment for Installation] in the same guide.
+
On large {Project} deployments, you can improve performance by configuring your {Project} with predefined tuning profiles.
For more information, see {InstallingProjectDocURL}tuning-with-predefined-profiles_{project-context}[Tuning {ProjectServer} with Predefined Profiles] in _{project-installation-guide-title}_.

* For more information about installing {ProjectServer} from a disconnected network, see https://access.redhat.com/documentation/en-us/red_hat_satellite/{ProductVersion}/html/installing_satellite_server_from_a_disconnected_network/[_Installing {ProjectServer} from a Disconnected Network_] and https://access.redhat.com/documentation/en-us/red_hat_satellite/{ProductVersion}/html/installing_satellite_server_from_a_disconnected_network/preparing-environment-for-installation[Preparing your Environment for Installation] in the same guide.
+
On large {Project} deployments, you can improve performance by configuring your {Project} with predefined tuning profiles.
For more information, see https://access.redhat.com/documentation/en-us/red_hat_satellite/{ProductVersion}/html/installing_satellite_server_from_a_disconnected_network/performing-additional-configuration#tuning-with-predefined-profiles_{project-context}[Tuning {ProjectServer} with Predefined Profiles] in _Installing {ProjectServer} from a Disconnected Network_.

.Adding {Project} Subscription Manifests to {ProjectServer}

A Subscription Manifest is a set of encrypted files that contains your subscription information.
{ProjectServer} uses this information to access the CDN and find what repositories are available for the associated subscription.
For instructions on how to create and import a Subscription Manifest see {ContentManagementDocURL}Managing_Subscriptions[Managing Subscriptions] in the _Content Management Guide_.

{ProjectName} requires a single manifest for each organization configured on the {Project}.
If you plan to use the Organization feature of {Project} to manage separate units of your infrastructure under one Red{nbsp}Hat{nbsp}Network account, then assign subscriptions from the one account to per-organization manifests as required.

If you plan to have more than one Red{nbsp}Hat{nbsp}Network account, or if you want to manage systems belonging to another entity that is also a Red{nbsp}Hat{nbsp}Network account holder, then you and the other account holder can assign subscriptions, as required, to manifests.
A customer that does not have a {Project} subscription can create a Subscription Asset Manager manifest, which can be used with {Project}, if they have other valid subscriptions.
You can then use the multiple manifests in one {ProjectServer} to manage multiple organizations.

If you must manage systems but do not have access to the subscriptions for the RPMs, you must use Red Hat Enterprise Linux Smart Management Add-On.
For more information, see https://www.redhat.com/en/store/smart-management-add#?sku=RH00031[Smart Management Add-On].

The following diagram shows two Red{nbsp}Hat{nbsp}Network account holders, who want their systems to be managed by the same {Project} installation.
In this scenario, Example Corporation 1 can allocate any subset of their 60 subscriptions, in this example they have allocated 30, to a manifest.
This can be imported into the {Project} as a distinct Organization.
This allows system administrators the ability to manage Example Corporation 1's systems using {Project} completely independently of Example Corporation 2's organizations (R&D, Operations, and Engineering).

[[satellite_server_with_multiple_manifests_image]]
.{ProjectServer} with Multiple Manifests
image::satellite_6_server_multiple_manifests.png[{ProjectServer} with Multiple Manifests]

When creating a Subscription Manifest:


* Add the subscription for {ProjectServer} to the manifest if planning a disconnected or self-registered {ProjectServer}.
This is not necessary for a connected {ProjectServer} that is subscribed using the Red{nbsp}Hat Subscription Manager utility on the base system.

* Add subscriptions for all {SmartProxyServer}s you want to create.

* Add subscriptions for all Red{nbsp}Hat Products you want to manage with {Project}.

* Note the date when the subscriptions are due to expire and plan for their renewal before the expiry date.

* Create one manifest per organization.
You can use multiple manifests and they can be from different Red Hat subscriptions.

{ProjectName} {ProductVersion} allows the use of future-dated subscriptions in the manifest.
This enables uninterrupted access to repositories when future-dated subscriptions are added to a manifest before the expiry date of existing subscriptions.

Note that the Subscription Manifest can be modified and reloaded to the {ProjectServer} in case of any changes in your infrastructure, or when adding more subscriptions.
Manifests should not be deleted.
If you delete the manifest from the Red Hat Customer Portal or in the {Project} Web UI it will unregister all of your content hosts.

endif::[]

[[satellite_server_with_external_database]]
=== {ProjectServer} with External Database

When you install {Project}, the `{foreman-installer}` command creates databases on the same server that you install {Project}.
Depending on your requirements, moving to external databases can provide increased working memory for {Project}, which can improve response times for database operating requests.
Moving to external databases distributes the workload and can increase the capacity for performance tuning.

Consider using external databases if you plan to use your {Project} deployment for the following scenarios:

* Frequent remote execution tasks.
This creates a high volume of records in PostgreSQL and generates heavy database workloads.
* High disk I/O workloads from frequent repository synchronization or Content View publishing.
This causes {Project} to create a record in PostgreSQL for each job.
* High volume of hosts.
* High volume of synced content.

For more information about using an external database, see {InstallingProjectDocURL}using-external-databases_{project-context}[Using External Databases with {Project}] in _{project-installation-guide-title}_.

[[sect-Mapping_the_Infrastructure_Topology]]
=== Locations and Topology

This section outlines general considerations that should help you to specify your {Project} deployment scenario.
The most common deployment scenarios are listed in xref:chap-{Project_Link}-Architecture_Guide-Deployment_Scenarios[].
The defining questions are:


* *How many {SmartProxyServer}s do I need?* – The number of geographic locations where your organization operates should translate to the number of {SmartProxyServer}s.
By assigning a {SmartProxy} to each location, you decrease the load on {ProjectServer}, increase redundancy, and reduce bandwidth usage.
{ProjectServer} itself can act as a {SmartProxy} (it contains an integrated {SmartProxy} by default).
This can be used in single location deployments and to provision the base system's of {SmartProxyServer}s.
Using the integrated {SmartProxy} to communicate with hosts in remote locations is not recommended as it can lead to suboptimal network utilization.

* *What services will be provided by {SmartProxyServer}s?* – After establishing the number of {SmartProxies}, decide what services will be enabled on each {SmartProxy}.
Even though the whole stack of content and configuration management capabilities is available, some infrastructure services (DNS, DHCP, TFTP) can be outside of a {Project} administrator's control.
In such case, {SmartProxies} have to integrate with those external services (see xref:{Project_Link}-Architecture_Guide-Capsule_with_External_Services[]).

ifeval::["{build}" == "satellite"]
* *Is my {ProjectServer} required to be disconnected from the Internet?* – Disconnected {Project} is a common deployment scenario (see xref:sect-{Project_Link}-Architecture_Guide-Disconnected_Satellite[]).
If you require frequent updates of Red{nbsp}Hat content on a disconnected {Project}, plan an additional {Project} instance for inter-{Project} synchronization.
endif::[]

* *What compute resources do I need for my hosts?* – Apart from provisioning bare metal hosts, you can use various compute resources supported by {Project}.
To learn about provisioning on different compute resources see the {ProvisioningDocURL}[_Provisioning Guide_].

[[sect-Defining_Content_Sources]]
=== Content Sources

The Subscription Manifest determines what Red{nbsp}Hat repositories are accessible from your {ProjectServer}.
Once you enable a Red{nbsp}Hat repository, an associated {Project} Product is created automatically.
For distributing content from custom sources you need to create products and repositories manually.
Red{nbsp}Hat repositories are signed with GPG keys by default, and it is recommended to create GPG keys also for your custom repositories.
The configuration of custom repositories depends on the type of content they hold (RPM packages, Puppet modules, Docker images, or OSTree snapshots).

Repositories configured as `yum` repositories, that contain only RPM packages, can make use of the new download policy setting to save on synchronization time and storage space.
This setting enables selecting from *Immediate*, *On demand*, and *Background*.
The *On demand* setting saves space and time by only downloading packages when requested by clients.
The *Background* setting saves time by completing the download after the initial synchronization.
For detailed instructions on setting up content sources see {ContentManagementDocURL}Importing_Red_Hat_Content[Importing Red Hat Content] in the _Content Management Guide_.

A custom repository within the {ProjectServer} is in most cases populated with content from an external staging server.
Such servers lie outside of the {Project} infrastructure, however, it is recommended to use a revision control system (such as Git) on these servers to have better control over the custom content.
[[sect-Defining_the_Content_Life_Cycle]]
=== Content Life Cycle

{Project} provides features for precise management of the content life cycle.
A *life cycle environment* represents a stage in the content life cycle, a *Content View* is a filtered set of content, and can be considered as a defined subset of content.
By associating Content Views with life cycle environments, you make content available to hosts in a defined way (see xref:figu-{Project_Link}-Architecture_Guide-{Project_Link}_6_System_Architecture-Content_Life_Cycle_in_{Project_Link}_6[] for visualization of the process).
For a detailed overview of the content management process see {ContentManagementDocURL}Importing_Custom_Content[Importing Custom Content] in the _Content Management Guide_.
The following section provides general scenarios for deploying content views as well as life cycle environments.

The default life cycle environment called *Library* gathers content from all connected sources.
It is not recommended to associate hosts directly with the Library as it prevents any testing of content before making it available to hosts.
Instead, create a life cycle environment path that suits your content workflow.
The following scenarios are common:


* *A single life cycle environment* – content from Library is promoted directly to the production stage.
This approach limits the complexity but still allows for testing the content within the Library before making it available to hosts.
+
image:lc_path-basic.png[A single life cycle environment]



* *A single life cycle environment path* – both operating system and applications content is promoted through the same path.
The path can consist of several stages (for example *Development*, *QA*, *Production*), which enables thorough testing but requires additional effort.
+
image:lc_path-simple.png[A single life cycle environment path]



* *Application specific life cycle environment paths* – each application has a separate path, which allows for individual application release cycles.
You can associate specific compute resources with application life cycle stages to facilitate testing.
On the other hand, this scenario increases the maintenance complexity.
+
image:lc_path-diverged.png[Application specific life cycle environment paths]



The following content view scenarios are common:


* *All in one content view* – a content view that contains all necessary content for the majority of your hosts.
Reducing the number of content views is an advantage in deployments with constrained resources (time, storage space) or with uniform host types.
However, this scenario limits the content view capabilities such as time based snapshots or intelligent filtering.
Any change in content sources affects a proportion of hosts.

* *Host specific content view* – a dedicated content view for each host type.
This approach can be useful in deployments with a small number of host types (up to 30).
However, it prevents sharing content across host types as well as separation based on criteria other than the host type (for example between operating system and applications).
With critical updates every content view has to be updated, which increases maintenance efforts.

* *Host specific composite content view* – a dedicated combination of content views for each host type.
This approach enables separating host specific and shared content, for example you can have a dedicated content view for Puppet configuration.
By including this content view into composite content views for several host types, you can update Puppet configuration with higher frequency than other host content.

* *Component based content view* – a dedicated content view for a specific application.
For example a database content view can be included into several composite content views.
This approach allows for greater standardization but it leads to an increased number of content views.

The optimal solution depends on the nature of your host environment.
Avoid creating a large number of content views, but keep in mind that the size of a content view affects the speed of related operations (publishing, promoting).
Also make sure that when creating a subset of packages for the content view, all dependencies are included as well.
Note that kickstart repositories should not be added to content views, as they are used for host provisioning only.

[[sect-Defining_Content_Deployment]]
=== Content Deployment

Content deployment is the management of errata and packages on content hosts.
There are two methods for content deployment on {Project}; the default is *Goferd service agent*, and a replacement is management via *remote execution*.

* *Goferd service agent* - The service communicates to and from the {Project} server and is primarily tasked with installing, updating, and reporting on packages.
It is enabled and started automatically on content hosts after successfully installing the *Katello-agent* RPM package.
+
Note that the Katello agent is deprecated and will be removed in a future {Project} version.

* *Remote execution* - Remote execution via SSH transport allows the install, update, or removal of packages, the bootstrap of configuration management agents, and the trigger of Puppet runs.
While the {ProjectServer} has remote execution enabled by default, it is disabled by default on {SmartProxyServer}s and content hosts and has to be manually enabled.

* *Consider method for content deployment* - The use of remote execution allows the *Goferd* service to be disabled, thereby reducing memory and CPU load on content hosts.
However, remote execution has to be manually configured on all content hosts before it can replace *Goferd*.
This configuration process is extensive for systems with large numbers of deployed content hosts.
For details, see {ManagingHostsDocURL}host-management-without-goferd-and-katello-agent_managing-hosts[Host Management Without Goferd and Katello Agent] in _Managing Hosts_.

[[sect-Automating_the_Provisioning]]
=== Provisioning

{Project} provides several features to help you automate the host provisioning, including provisioning templates, configuration management with Puppet, and host groups for standardized provisioning of host roles.
For a description of the provisioning workflow see {ProvisioningDocURL}provisioning-workflow_provisioning[Provisioning Workflow] in the _Provisioning Guide_.
The same guide contains instructions for provisioning on various compute resources.

[[sect-Defining_Role_Based_Authentication]]
=== Role Based Authentication

Assigning a role to a user enables controlling access to {Project} components based on a set of permissions.
You can think of role based authentication as a way of hiding unnecessary objects from users who are not supposed to interact with them.

There are various criteria for distinguishing among different roles within an organization.
Apart from the administrator role, the following types are common:


* *Roles related to applications or parts of infrastructure* – for example, roles for owners of Red Hat Enterprise Linux as the operating system versus owners of application servers and database servers.

* *Roles related to a particular stage of the software life cycle* – for example, roles divided among the development, testing, and production phases, where each phase has one or more owners.

* *Roles related to specific tasks* – such as security manager or license manager.

When defining a custom role, consider the following recommendations:


* *Define the expected tasks and responsibilities* – define the subset of the {Project} infrastructure that will be accessible to the role as well as actions permitted on this subset.
Think of the responsibilities of the role and how it would differ from other roles.

* *Use predefined roles whenever possible* – {Project} provides a number of sample roles that can be used alone or as part of a role combination.
Copying and editing an existing role can be a good start for creating a custom role.

* *Consider all affected entities* – for example, a content view promotion automatically creates new Puppet Environments for the particular life cycle environment and content view combination.
Therefore, if a role is expected to promote content views, it also needs permissions to create and edit Puppet Environments.

* *Consider areas of interest* – even though a role has a limited area of responsibility, there might be a wider area of interest.
Therefore, you can grant the role a read only access to parts of {Project} infrastructure that influence its area of responsibility.
This allows users to get earlier access to information about potential upcoming changes.

* *Add permissions step by step* – test your custom role to make sure it works as intended.
A good approach in case of problems is to start with a limited set of permissions, add permissions step by step, and test continuously.


For instructions on defining roles and assigning them to users, see {AdministeringDocURL}chap-Red_Hat_Satellite-Administering_Red_Hat_Satellite-Users_and_Roles[Managing Users and Roles] in the _Administering {ProjectName}_ guide.
The same guide contains information on configuring external authentication sources.

[[sect-Additional_Tasks]]
=== Additional Tasks

This section provides a short overview of selected {Project} capabilities that can be used for automating certain tasks or extending the core usage of {Project}:

ifeval::["{build}" == "satellite"]
* *Importing existing hosts* – if you have existing hosts that have not been managed by {Project} in the past, you can import those hosts to the {ProjectServer}.
This procedure is usually a step in transitioning from {ProjectName} 5.
For more information, see https://access.redhat.com/documentation/en-us/red_hat_satellite/{ProductVersion}/html/transitioning_from_red_hat_satellite_5_to_red_hat_satellite_6/index[_Transitioning from {ProjectName} 5 to {ProjectNameX}_] for detailed documentation.
A high level overview of the transition process is available in the Red{nbsp}Hat Knowledgebase solution https://access.redhat.com/articles/1187643[Transitioning from Red Hat Satellite 5 to Satellite 6].
endif::[]

* *Discovering bare metal hosts* – the {Project} Discovery plug-in enables automatic bare-metal discovery of unknown hosts on the provisioning network.
These new hosts register themselves to the {ProjectServer} and the Puppet Agent on the client uploads system facts collected by Facter, such as serial ID, network interface, memory, and disk information.
After registration you can initialize provisioning of those discovered hosts.
For details, see {ProvisioningDocURL}creating-hosts-from-discovered-hosts[Creating Hosts from Discovered Hosts] in the _Provisioning Guide_.

* *Backup management* – backup and disaster recovery instructions, see {AdministeringDocURL}backing-up-satellite-server-and-capsule-server[Backing Up {ProjectServer} and {SmartProxyServer}] in _Administering {ProjectName}_.
Using remote execution, you can also configure recurring backup tasks on managed hosts.
For more information on remote execution see {ManagingHostsDocURL}configuring-and-setting-up-remote-jobs_managing-hosts[Configuring and Setting up Remote Jobs] in _Managing Hosts_.

* *Security management* – {Project} supports security management in various ways, including update and errata management, OpenSCAP integration for system verification, update and security compliance reporting, and fine grained role based authentication.
Find more information on errata management and OpenSCAP concepts in {ManagingHostsDocURL}[_Managing Hosts_].

* *Incident management* – {Project} supports the incident management process by providing a centralized overview of all systems including reporting and email notifications.
Detailed information on each host is accessible from the {ProjectServer}, including the event history of recent changes.
ifeval::["{build}" == "satellite"]
{Project} is also integrated with https://access.redhat.com/products/red-hat-insights/#sat6[Red{nbsp}Hat Insights].
endif::[]

* *Scripting with Hammer and API* – {Project} provides a command line tool called Hammer that provides a CLI equivalent to the majority of web UI procedures.
In addition, you can use the access to the {Project} API to write automation scripts in a selected programming language.
For more information see the https://access.redhat.com/documentation/en-us/red_hat_satellite/{ProductVersion}/html/hammer_cli_guide/[_Hammer CLI Guide_] and https://access.redhat.com/documentation/en-us/red_hat_satellite/{ProductVersion}/html/api_guide/[_API Guide_].
