[[Provisioning_Virtual_Machines_in_GCE]]
== Provisioning Cloud Instances in Google Compute Engine
{ProjectName} can interact with Google Compute Engine (GCE), including creating new virtual machines and controlling their power management states. Only image-based provisioning is supported for creating GCE hosts.

[[Provisioning_Virtual_Machines_in_GCE-Prerequisites_for_GCE_Provisioning]]
=== Prerequisites for GCE Provisioning

Before you begin, ensure that the following conditions are met:

* In your GCE project, configure a service account with the necessary IAM Compute role. For more information, see https://cloud.google.com/compute/docs/access/iam[Compute Engine IAM roles ] in the GCE documentation.
* In your GCE project-wise metadata, set the `enable-oslogin` to `FALSE`. For more information, see https://cloud.google.com/compute/docs/instances/managing-instance-access#enable_oslogin[Enabling or disabling OS Login] in the GCE documentation.
* Optional: If you want to use Puppet with GCE hosts, navigate to *Administer* > *Settings* > *Puppet* and enable the `Use UUID for certificates` setting to configure Puppet to use consistent Puppet certificate IDs.
* Based on your needs, associate a `finish` or `user_data` provisioning template with the operating system you want to use. For more information about provisioning templates, see xref:Configuring_Provisioning_Resources-Types_of_Provisioning_Templates[].
include::Common_Compute_Resource_Prereqs.adoc[]

[[Provisioning_Virtual_Machines_in_GCE-Adding_a_GCE_Connection]]
=== Adding a GCE Connection to {ProjectServer}

Use this procedure to add a GCE connection to {Project} to be able to add images and provision hosts on GCE.

.Procedure

. In GCE, generate a service account key in JSON format and upload this file to the `/usr/share/foreman/` directory on {ProjectServer}.
. On {ProjectServer}, change owner for the service account key to the `foreman` user:
+
[options="nowrap" subs="+quotes"]
----
# chown foreman /usr/share/foreman/_gce_key_.json
----
. Configure permissions for the service account key to ensure that the file is readable:
+
[options="nowrap" subs="+quotes"]
----
# chmod 0600 /usr/share/foreman/_gce_key_.json
----
. Restore SELinux context for the service account key:
+
[options="nowrap" subs="+quotes"]
----
# restorecon -vv /usr/share/foreman/_gce_key_.json
----
. In the {Project} web UI, navigate to *Infrastructure* > *Compute Resources* and click *Create Compute Resource*.
. In the *Name* field, enter a name for the resource.
. From the *Provider* list, select *Google*.
. Optional: In the *Description* field, enter a description for the resource.
. In the *Google Project ID* field, enter the project ID.
. In the *Client Email* field, enter the client email.
. In the *Certificate Path* field, enter the path to the service account key. For example, `/usr/share/foreman/_gce_key_.json`.
. Click *Load Zones* to populate the list of zones from your GCE environment.
. From the *Zone* list, select the GCE zone to use.
. Click *Submit*.

.For CLI Users

. In GCE, generate a service account key in JSON format and upload this file to the `/usr/share/foreman/` directory on {ProjectServer}.

. On {ProjectServer}, change owner for the service account key to the `foreman` user:
+
[options="nowrap" subs="+quotes"]
----
# chown foreman /usr/share/foreman/_gce_key_.json
----

. Configure permissions for the service account key to ensure that the file is readable:
+
[options="nowrap" subs="+quotes"]
----
# chmod 0600 /usr/share/foreman/_gce_key_.json
----

. Restore SELinux context for the service account key:
+
[options="nowrap" subs="+quotes"]
----
# restorecon -vv /usr/share/foreman/_gce_key_.json
----

. Use the `hammer compute-resource create` command to add a GCE compute resource to {Project}.
+
[options="nowrap" subs="+quotes"]
----
# hammer compute-resource create --name '_gce_cr_' \
--provider 'gce' \
--project '_gce_project_id_' \
--key-path '_gce_key_.json' \
--zone '_us-west1-b_' \
--email '_gce_email_'
----

[[Provisioning_Virtual_Machines_in_GCE-Adding_GCE_Images]]
=== Adding GCE Images to {ProjectServer}

GCE uses image-based provisioning to create hosts. You must add image details to your {ProjectServer}.

.Procedure

. In the {Project} web UI, navigate to *Infrastructure* > *Compute Resources* and click the GCE connection.
. Click the *Images* tab.
. Click *Create Image*.
. In the *Name* field, enter a name for the image.
. From the *Operating system* list, select the base operating system for the image.
. From the *Architecture* list, select the operating system architecture.
. In the *User* field, enter the SSH user name for image access. Specify a user other than `root`, because the `root` user cannot connect to a GCE instance using SSH keys. The username must begin with a letter and consist of lowercase letters and numbers.
. From the *Image* list, select the GCE image.
. Optional: If the selected image supports `cloud-init`, select the *User data* check box to enable user data input.
. Click *Submit* to save the image details.

.For CLI Users

Use the `hammer compute-resource image create` command to add an image to the GCE compute resource. With the `--username` option, specify a user other than `root`, because the `root` user cannot connect to a GCE instance using SSH keys. The username must begin with a letter and consist of lowercase letters and numbers.

[options="nowrap" subs="+quotes"]
----
# hammer compute-resource image create --name '_gce_image_name_' \
--compute-resource '_gce_cr_' \
--operatingsystem-id 1 \
--architecture-id 1 \
--uuid '_378010XXXXXXXXX_' \
--username '_admin_'
----

[[Provisioning_Virtual_Machines_in_GCE-Adding_GCE_Details_to_a_Compute_Profile]]
=== Adding GCE Details to a Compute Profile

You can add GCE details to a compute profile to automatically populate virtual machine-based settings on host creation.

.Procedure

To add details to a compute profile, complete the following steps:

. In the {Project} web UI, navigate to *Infrastructure* > *Compute Profiles* and click the name of your profile.
. Select a GCE connection.
. From the *Machine Type* list, select the machine type to use for provisioning.
. From the *Image* list, select the image to use for provisioning.
. From the *Network* list, select the GCE network to use for provisioning.
. Optional: Select the *Associate Ephemeral External IP* check box to assign a dynamic ephemeral IP address that {Project} uses to communicate with the host. This public IP address changes when you reboot the host. If you need a permanent IP address, reserve a static public IP address on GCE and attach it to the host.
. In the *Size (GB)* field, enter the size of the storage to create on the host.
. Click *Submit* to save the compute profile.

.For CLI Users

. Create a compute profile to use with the GCE compute resource:
+
[options="nowrap" subs="+quotes"]
----
# hammer compute-profile create --name _gce_profile_
----

. Add GCE details to the compute profile. 
+
[options="nowrap" subs="+quotes"]
----
# hammer compute-profile values create --compute-profile _gce_profile_ \
--compute-resource '_gce_cr_' \
--volume "_size_gb=20_" \
--compute-attributes "machine_type=f1-micro,associate_external_ip=true,network=default" 
----

[[Provisioning_Virtual_Machines_in_GCE-Creating_Image_Based_Hosts_on_GCE]]
=== Creating Image-Based Hosts on GCE

The GCE provisioning process creates hosts from existing images on GCE.

.Procedure

To create a host on GCE, complete the following steps:

. In the {Project} web UI, navigate to *Hosts* > *Create host*.
. In the *Name* field, enter a name for the host.
. Click the *Organization* and *Location* tabs to ensure that the provisioning context is automatically set to the current context.
. Optional: From the *Host Group* list, you can select a host group to populate most of the new host's fields.
. From the *Deploy on* list, select the GCE connection.
. From the *Compute Profile* list, select a profile to use to automatically populate virtual machine-based settings.
. Click the *Interface* tab and click *Edit* on the host's interface.
. Verify that the fields are automatically populated with values, particularly the following items:
+
  * The *Name* from the *Host* tab becomes the *DNS name*.
  * The *MAC address* field is blank.
  * The *Domain* field is populated with the required domain.
  * The *Managed*, *Primary*, and *Provision* options are automatically selected for the first interface on the host. If not, select them.
+
. Click the *Operating System* tab, and select the operating system to install.
. Click *Resolve* in *Provisioning templates* to verify that the new host can identify the correct provisioning templates to use. You must select either a `finish` or `user_data` provisioning template.
ifeval::["{build}" == "satellite"]
. Click the *Parameters* tab and ensure that a parameter exists that provides an activation key. If not, add an activation key.
endif::[]
ifeval::["{build}" == "foreman"]
. If you use the Katello plugin, click the *Parameters* tab and ensure that a parameter exists that provides an activation key. If not, add an activation key.
endif::[]
. Click *Submit* to save the host entry.

.For CLI Users

To create a host, enter the following command:

[options="nowrap" subs="+quotes"]
----
# hammer host create --name "_GCE_VM_" \
--organization "_Your_Organization_" \
--location "_Your_Location_" \
--compute-resource _gce_cr_name_
--compute-profile "_gce_profile_name_" \
--image _gce_image_name_
--provision-method 'image' \
--root-password "_your_root_password_" \
--interface "type=interface,domain_id=1,managed=true,primary=true,provision=true" \
--puppet-environment-id _1_ \
--puppet-ca-proxy-id _1_ \
--puppet-proxy-id _1_ \
--architecture _x86_64_ \
--operatingsystem "_operating_system_name_" \
----
